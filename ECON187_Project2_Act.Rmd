---
title: "ECON 187 Project 2"
author: "Arif Abd Aziz, Jon Girolamo, Shanie Talor"
date: "2023-05-18"
output: pdf_document
---

#Notes #pull #add/save #commit #pull(to get someone elses)/push (to give yours to everyone else)

# Part I: Non-Linear Models

## Introduction

The data we found explores the relationship between the fat content of multiple different food items against the incidence and prevalence of COVID 19. The data is aggregated from 170 countries (170 observations), and we plan to use a variety of non-linear techniques to create a model that predicts COVID contraction rates from aggregated nutrition variables (pertaining to food fat content).

## Read & Clean Covid Data:
```{r}
# Clear Environment:
rm(list = ls())

# Read Data into Environment
nutrition <- read.csv("Fat_Supply_Quantity_Data.csv", sep = ",",
                      header = TRUE)
head(nutrition)

# See Variable Names
names(nutrition)

# Distribution/Histogram of Quantitative Variables: Broken into 3 Parts
# First nine variables
par(mfrow = c(3,3)) # Break this up because need plots to look clean
for (variable in names(nutrition)[1:9]) {
  if (is.numeric(nutrition[[variable]])) {
    hist(nutrition[[variable]], main = variable, xlab = paste0(variable," values"))
  }
}

# Second group of nine variables
par(mfrow = c(3,3))
for (variable in names(nutrition)[10:18]) {
  if (is.numeric(nutrition[[variable]])) {
    hist(nutrition[[variable]], main = variable, xlab = paste0(variable," values"))
  }
}

# Last Group of Nine Variables
par(mfrow = c(3,3))
for (variable in names(nutrition)[19:27]) {
  if (is.numeric(nutrition[[variable]])) {
    hist(nutrition[[variable]], main = variable, xlab = paste0(variable," values"))
  }
}

# Last Group of 5 Variables
par(mfrow = c(3,2))
for (variable in names(nutrition)[28:32]) {
  if (is.numeric(nutrition[[variable]])) {
    hist(nutrition[[variable]], main = variable, xlab = paste0(variable," values"))
  }
}
```

There are a lot of predictors in this data set, representing the fat contents of various foods. We want to model for variables that show a high degree of variation. We can pick out these predictors from their histograms. Those that are concentrated at certain values may not give us the insights into predicting the incidence of COVID-19 in different countries. For example, aquatic products and their fat contents don't exhibit strong variation. This makes sense since some countries may not have aquatic products. Though this variable may help to predict COVID 19 incidence in countries that do consume aquatic products, it may pick up dynamics in non-aquatic consuming countries that do not exist in the data.

We also want to remove variables that may contain similar information to others, such as "Sugar Crops" and "Sugar Sweetners". Our main response variables are COVID-19 deaths and confirmed cases. These represent proportions of a country's total population.

```{r}
# Keep Variables That We'd Like to Study 
library(dplyr)
nutrition <- nutrition %>% select(-c("Country", "Alcoholic.Beverages",
                                     "Aquatic.Products..Other", "Oilcrops",
                                     "Sugar.Crops", "Population",
                                     "Unit..all.except.Population.", "Recovered",
                                     "Active"))

# Remove Empty NA Values, Assuming Death Rate is Our Main Response Variable
removena <- which(is.na(nutrition$Deaths))
nutrition <- nutrition[-removena,]
nrow(nutrition)

# Check to See if Any Other NA Values in Main Variables:
for (variable in names(nutrition)){
  print(paste0(variable," has NA values: ",any(is.na(nutrition[[variable]]))))
} # See That Obesity and Undernourished Variables Still Have NA Values

removena_obesity <- which(is.na(nutrition$Obesity))
removena_undernourished <- which(is.na(nutrition$Undernourished))

# Remove Further NA Values
nutrition <- nutrition[-removena_obesity,]
nutrition <- nutrition[-c(5,11,59,120,124,125,137,144),]
nrow(nutrition)
nutrition <- na.omit(nutrition)
nrow(nutrition) 
```

Now that our data set is largely cleaned, removing for predictors we are not interested in and adjusting for empty values, we can now look into feature selection methods. The main one we will use is the Boruta algorithm which uses random forests to pick out a subset of best predictors. We will assume that Covid-19 deaths is our response variable.

## Feature Selection

```{r, echo = FALSE}
# Boruta Algorithm
library(Boruta)
library(randomForest)
boruta_covid <- Boruta(Deaths~.-Confirmed, data = nutrition, doTrace = 2, 
                       randomForest = TRUE)
```

```{r}
# Boruta Plot
plot(boruta_covid, main = "Boruta Algorithm Feature Selection", las = 2)
```

According to our Boruta plot, ten out of the twenty three predictors we shortlisted are significant in explaining and predicting COVID-19 deaths. Moving forward, we'll be using these 10 predictors and applying specific transformations of them (splines, polynomials) to predict COVID-19 deaths.

Here are the best predictors according to Boruta: "Stimulants", "Pulses", "Vegetable.Oils", "Miscellaneous", "Undernourished", "Milk...Excluding.Butter", "Fish..Seafood", "Vegetal.Products", "Animal.Products", "Animal.fats", "Eggs", "Obesity".

# Relationship Between Response Variable & Chosen Predictors

```{r}
# Keep Variables We Want to Model
nutrition_cln <- nutrition %>% select(-c("Fruits...Excluding.Wine","Spices",
                                         "Sugar...Sweeteners","Vegetables",
                                         "Offals","Treenuts","Meat",
                                         "Starchy.Roots", "Confirmed"))


# Convert Undernourished into Numerical Variable
nutrition_cln$Undernourished[nutrition_cln$Undernourished == "<2.5"] <- 2.5
nutrition_cln$Undernourished <- as.numeric(nutrition_cln$Undernourished) # make numeric

# Create For Function to See Plots of All Predictors Against Covid 19 Deaths
for (variable in names(nutrition_cln)){
  plot(nutrition_cln[[variable]], nutrition_cln$Deaths, main = paste0("COVID 19 Deaths Against ",variable), xlab = variable, ylab = "Population Proportion of Deaths")
}
```

# Creating folds for all Cross-Validation Testing

```{r}
library(caret)
set.seed(123)
folds <- createFolds(nutrition$Deaths, k = 5, returnTrain = TRUE)

```

## Piecewise Polynomial: Shanie (Leave Descriptions Until Last, Just Write Code)

```{r}
variable_list <- list() # Empty List to Store Performance Scores for Each Predictor

# Define the combinations of predictors
predictor_combinations <- list(
  c("Stimulants"),
  c("Pulses"),
  c("Vegetable.Oils"),
  c("Miscellaneous"),
  c("Undernourished"),
  c("Milk...Excluding.Butter"),
  c("Fish..Seafood"),
  c("Vegetal.Products"),
  c("Animal.Products"),
  c("Animal.fats"),
  c("Eggs"),
  c("Obesity"),
  c("Vegetal.Products", "Animal.Products", "Undernourished"),
  c("Vegetal.Products", "Animal.Products"),
  c("Vegetal.Products", "Undernourished"),
  c("Animal.Products", "Undernourished"),
  c("Vegetal.Products", "Animal.Products", "Undernourished", "Stimulants"),
  c("Vegetal.Products", "Animal.Products", "Undernourished", "Pulses"),
  c("Vegetal.Products", "Animal.Products", "Undernourished", "Vegetable.Oils"),
  c("Vegetal.Products", "Animal.Products", "Undernourished", "Miscellaneous"),
  c("Vegetal.Products", "Animal.Products", "Undernourished", "Milk...Excluding.Butter"),
  c("Vegetal.Products", "Animal.Products", "Undernourished", "Fish..Seafood"),
  c("Vegetal.Products", "Animal.Products", "Undernourished", "Eggs"),
  c("Vegetal.Products", "Animal.Products", "Undernourished", "Obesity"),
  c("Vegetal.Products", "Animal.Products", "Stimulants"),
  c("Vegetal.Products", "Animal.Products", "Pulses"),
  c("Vegetal.Products", "Animal.Products", "Vegetable.Oils"),
  c("Vegetal.Products", "Animal.Products", "Miscellaneous"),
  c("Vegetal.Products", "Animal.Products", "Milk...Excluding.Butter"),
  c("Vegetal.Products", "Animal.Products", "Fish..Seafood"),
  c("Vegetal.Products", "Animal.Products", "Eggs"),
  c("Vegetal.Products", "Animal.Products", "Obesity"),
  c("Vegetal.Products", "Undernourished", "Stimulants"),
  c("Vegetal.Products", "Undernourished", "Pulses"),
  c("Vegetal.Products", "Undernourished", "Vegetable.Oils"),
  c("Vegetal.Products", "Undernourished", "Miscellaneous"),
  c("Vegetal.Products", "Undernourished", "Milk...Excluding.Butter", "Fish..Seafood"),
  c("Vegetal.Products", "Undernourished", "Milk...Excluding.Butter", "Eggs"),
  c("Vegetal.Products", "Undernourished", "Milk...Excluding.Butter", "Obesity"),
  c("Vegetal.Products", "Undernourished", "Fish..Seafood", "Eggs"),
  c("Vegetal.Products", "Undernourished", "Fish..Seafood", "Obesity"),
  c("Vegetal.Products", "Undernourished", "Eggs", "Obesity")
)

# Iterate over predictor combinations
for (predictors in predictor_combinations) {
  cv_num_vector <- 1:4 # First Column of Data to Represent Degrees of Freedom
  cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
  emptyvector <- vector() # Vector for Mean Squared Error Scores
  
  for (i in 1:length(folds)) {
    train_index <- folds[[i]]
    test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
    nutrition_test <- nutrition_cln[test_index,]
    
    for (j in 1:4) {
      formula <- paste("Deaths ~ poly(", paste(predictors, collapse = "+"), 
                       ", degree = ", j, ", raw = TRUE)", sep = "")
      fit <- lm(formula, data = nutrition_cln, subset = train_index) # Fit Piecewise Polynomial with specified degree
      prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
      mse_score <- mean((nutrition_test$Deaths - prediction)^2) # MSE Calculation
      emptyvector[j-3] <- round(mse_score, 6)
    }
    
    cv_error_df <- cbind(cv_error_df, emptyvector) # Add MSE from DF into Data Frame
    emptyvector <- vector() # Empty Vector
  }
  
  # Clean Data Frame & Return Values to Main Data Frame
  colnames(cv_error_df) <- c("Degrees of Freedom", "Fold 1", "Fold 2", "Fold 3", "Fold 4", "Fold 5")
  cv_error_df$Means <- round(rowMeans(cv_error_df[, 2:6]), 6)
  print(cv_error_df)
  
  # Store results in variable_list
  variable_list[[paste(predictors, collapse = "_")]] <- c(cv_error_df[which.min(cv_error_df$Means), 1], min(cv_error_df$Means))
}

# Print the results
print(variable_list)

# Find the top 3 minimum MSE values
top3_min_mse <- sort(sapply(variable_list, function(x) x[2]))[1:3]

# Print the elements of variable_list where MSE is in the top 3 minimum values
for (variable in names(variable_list)) {
  mse <- variable_list[[variable]][2]
  if (mse %in% top3_min_mse) {
    print(paste("Variable:", variable))
    print(paste("Degree of Freedom:", variable_list[[variable]][1]))
    print(paste("MSE:", mse))
  }
}
```

//// will remove this ////Looking at the piecewise polynomial model of different combinations of the predictors againt the variable "Deaths" we can see that of the independent predictors used, "Undernourished" had the lowest MSE of 0.001729 and an optimal degree of freedom being 4. The independent predictor with the highest MSE was the "Fish..Seafood" with an MSE of 0.173299 and a degree of freedom of 4. Looking at the combinations of predictors we can see that the piecewise polynomial with predictors "Egg"+ "Obesity"+ "Animal.fats" has the lowest MSE of 0.001589 and a degree of freedom of 4, appearing to be the best model of the combinations provided. /////

Top individual Predictors
Vegetal.Products
Degrees of Freedom: 1
MSE: 0.001691

Animal.Products
Degrees of Freedom: 1
MSE: 0.001691

Undernourished
Degrees of Freedom: 1
MSE: 0.001729

Top combined Predictor
[1] "Variable: Vegetal.Products_Animal.Products_Undernourished_Fish..Seafood"
[1] "Degree of Freedom: 1"
[1] "MSE: 0.001663"



Top 3 overall models: 

[1] "Variable: Vegetal.Products"
[1] "Degree of Freedom: 1"
[1] "MSE: 0.001691"

[1] "Variable: Animal.Products"
[1] "Degree of Freedom: 1"
[1] "MSE: 0.001691"

[1] "Variable: Vegetal.Products_Animal.Products_Undernourished_Fish..Seafood"
[1] "Degree of Freedom: 1"
[1] "MSE: 0.001663"

## Splines: Arif (Leave Descriptions Until Last, Just Write Code)

Since we have many predictors to account for, I'll be optimizing each predictor against our response variable (COVID-19 deaths) individually and separately. In order to fit splines to our data, We'll be using a matrix of spline functions, or basis functions (bs()). Though we could individually set knots for each variable and test its general performance, we do not have a strong understanding of nutritional theory. Thus, instead, we'll use cross-validation techniques to optimize the degrees of freedom for each predictor and their corresponding basis functions. The degrees of freedom take into account the best knots for the predictors. Instead of mixing and matching different predictors and their basis functions, we'll be working on each predictor's basis functions individually, then compiling the best based on cross-validation performance.

```{r}
# Cross Validation for Each Variable Using Splines and Basis Functions 
library(splines)
variable_list <- list() # Empty List to Store Performance Scores for Each Predictor

### Animal Products Test ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Animal.Products, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Animal.Products Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Animal.Products"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means))

### Animal Fats Test ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Animal.fats, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Animal.fats Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Animal.fats"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means))

### Cereals Excluding Beer ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Cereals...Excluding.Beer, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Cereals...Excluding.Beer Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Cereals...Excluding.Beer"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means))

### Eggs ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Eggs, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Eggs Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Eggs"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means))

### Fish Seafood ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Fish..Seafood, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Fish..Seafood Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Fish..Seafood"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means))

### Miscellaneous ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Miscellaneous, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Miscellaneous Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Miscellaneous"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means))

### Milk...Excluding.Butter Seafood ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Milk...Excluding.Butter, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Milk...Excluding.Butter Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Milk...Excluding.Butter"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means)) 

### Pulses ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Pulses, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Pulses Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Pulses"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means)) 

### Stimulants ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Stimulants, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Stimulants Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Stimulants"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means)) 

### Vegetal.Products ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Vegetal.Products, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Vegetal.Products Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Vegetal.Products"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means)) 

### Vegetable.Oils ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Vegetable.Oils, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Vegetable.Oils Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Vegetable.Oils"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means)) 

### Obesity ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Obesity, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Obesity Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Obesity"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means)) 

### Undernourished ###
cv_num_vector <- 4:15 # First Column of Data to Represent Degrees of Freedom
cv_error_df <- data.frame(cv_num_vector) # Put Column into Data Frame
emptyvector <- vector() # Vector for Mean Squared Error Scores

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  for (j in 4:15){
    fit <- lm(Deaths ~ ns(Undernourished, df = j), data = nutrition_cln, 
              subset = train_index) # Fit Natural Spline (Cubic) Varying DFs
    prediction <- predict(fit, newdata = nutrition_test) # Predict on Test Set
    mse_score <- mean((nutrition_test$Deaths-prediction)^2) # MSE Calculation
    emptyvector[j-3] <- round(mse_score,6)
  }
  cv_error_df <- cbind(cv_error_df,emptyvector) # Add MSE from DF into Data Frame
  emptyvector <- vector() # Empty Vector
}

# Clean Data Frame & Return Values to Main Data Frame
colnames(cv_error_df) <- c("Degrees of Freedom","Fold 1", "Fold 2", "Fold 3", 
                           "Fold 4", "Fold 5")
cv_error_df$Means <- round(rowMeans(cv_error_df[,2:6]),6)
print(cv_error_df)
print(paste0("Minimum Degree of Freedom for Undernourished Natural Spline: ",
             cv_error_df[which.min(cv_error_df$Means),1])) 
variable_list[["Undernourished"]] <- c(cv_error_df[which.min(cv_error_df$Means),1],
                             min(cv_error_df$Means)) 

### Print Full Variable List ####
print(variable_list) # Natural Splines Fit for Individual Variables
```

Looking at our full list of variables and their natural spline performance, we see that the natural splines that returned the lowest MSE scores were: Obesity, Undernourished, Eggs, Animal Products, Vegetal Products, and Animal Fats. All of these natural splines produced an MSE score of below 0.002. Moreover, we ran for loops that tested degrees of freedom between 4 and 15 and surprisingly most of our optimal degrees of freedom for each individual variable are below 6. This in theory reduces potential overfitting in testing sets. Moving forward, we will consider potential combinations of these splines, emphasizing the variables that performed the best in our (natural spline) cross validation exercise. In order to figure out which combinations of variables are optimal, we'll be using a cross-validation approach using the same folds set before. We'll be comparing MSE and AIC/BIC scores. Though there may be more robust algorithms that can optimize a variety of combinations of variables, we'll be using an approach similar to forward step-wise selection. In this version, we'll be adding the best natural spline variables in order of their MSE performance from the previous section.

Order: Obesity, Eggs, Undernourished, Animal Products, Vegetal Products, Animal Fats

```{r}
###### Assess & Try Different Combinations of Natural Splines #####
mse_vector_ns <- vector()
bic_vector_ns <- vector()
model_comp_df <- data.frame()

# Model 1 Natural Splines: Obesity Only
for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  
  # Model Fit & Predict on Testing Data
  model <- lm(Deaths ~ ns(Obesity,4), data = nutrition_cln, subset = train_index)
  predictions <- predict(model, newdata = nutrition_test) 
  
  # MSE Calculation & Input into Vector
  mse_score <- mean((nutrition_test$Deaths-predictions)^2) 
  mse_vector_ns[i] <- mse_score
  
  # BIC Calculation
  n <- nrow(nutrition_test) # For AIC/BIC Calculations
  p <- length(model$coefficients) # Number of Parameters, Will Change Between Models
  bic_vector_ns[i] <- (n*log(mse_score))+(p*log(n))
}

model1 <- c(mean(mse_vector_ns),mean(bic_vector_ns)) 
model_comp_df <- data.frame(model1)
rownames(model_comp_df) <- c("MSE", "BIC")  

# Model 2 Natural Splines: Obesity, Eggs

mse_vector_ns <- vector() # Reset Vectors
bic_vector_ns <- vector()

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  
  # Model Fit & Predict on Testing Data
  model <- lm(Deaths ~ ns(Obesity,4)+ns(Eggs,4), data = nutrition_cln, 
              subset = train_index)
  predictions <- predict(model, newdata = nutrition_test) 
  
  # MSE Calculation & Input into Vector
  mse_score <- mean((nutrition_test$Deaths-predictions)^2) 
  mse_vector_ns[i] <- mse_score
  
  # BIC Calculation
  n <- nrow(nutrition_test) # For AIC/BIC Calculations
  p <- length(model$coefficients) # Number of Parameters, Will Change Between Models
  bic_vector_ns[i] <- (n*log(mse_score))+(p*log(n))
}

model2 <- c(mean(mse_vector_ns),mean(bic_vector_ns)) 
model_comp_df <- cbind(model_comp_df, model2)

# Model 3 Natural Splines: Obesity, Eggs, Undernourished

mse_vector_ns <- vector() # Reset Vectors
bic_vector_ns <- vector()

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  
  # Model Fit & Predict on Testing Data
  model <- lm(Deaths ~ ns(Obesity,4)+ns(Eggs,4)+ns(Undernourished,4), 
              data = nutrition_cln, subset = train_index)
  predictions <- predict(model, newdata = nutrition_test) 
  
  # MSE Calculation & Input into Vector
  mse_score <- mean((nutrition_test$Deaths-predictions)^2) 
  mse_vector_ns[i] <- mse_score
  
  # BIC Calculation
  n <- nrow(nutrition_test) # For AIC/BIC Calculations
  p <- length(model$coefficients) # Number of Parameters, Will Change Between Models
  bic_vector_ns[i] <- (n*log(mse_score))+(p*log(n))
}

model3 <- c(mean(mse_vector_ns),mean(bic_vector_ns)) 
model_comp_df <- cbind(model_comp_df, model3)

# Model 4 Natural Splines: Obesity, Eggs, Undernourished, Animal Products

mse_vector_ns <- vector() # Reset Vectors
bic_vector_ns <- vector()

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  
  # Model Fit & Predict on Testing Data
  model <- lm(Deaths ~ ns(Obesity,4)+ns(Eggs,4)+ns(Undernourished,4)+
                ns(Animal.Products,5), data = nutrition_cln, subset = train_index)
  predictions <- predict(model, newdata = nutrition_test) 
  
  # MSE Calculation & Input into Vector
  mse_score <- mean((nutrition_test$Deaths-predictions)^2) 
  mse_vector_ns[i] <- mse_score
  
  # BIC Calculation
  n <- nrow(nutrition_test) # For AIC/BIC Calculations
  p <- length(model$coefficients) # Number of Parameters, Will Change Between Models
  bic_vector_ns[i] <- (n*log(mse_score))+(p*log(n))
}

model4 <- c(mean(mse_vector_ns),mean(bic_vector_ns)) 
model_comp_df <- cbind(model_comp_df, model4)

# Model 5 Natural Splines: Obesity, Eggs, Undernourished, Ani Prod, Veg Products

mse_vector_ns <- vector() # Reset Vectors
bic_vector_ns <- vector()

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  
  # Model Fit & Predict on Testing Data
  model <- lm(Deaths ~ ns(Obesity,4)+ns(Eggs,4)+ns(Undernourished,4)+
                ns(Animal.Products,5)+ns(Vegetal.Products,5), 
              data = nutrition_cln, subset = train_index)
  predictions <- predict(model, newdata = nutrition_test) 
  
  # MSE Calculation & Input into Vector
  mse_score <- mean((nutrition_test$Deaths-predictions)^2) 
  mse_vector_ns[i] <- mse_score
  
  # BIC Calculation
  n <- nrow(nutrition_test) # For AIC/BIC Calculations
  p <- length(model$coefficients) # Number of Parameters, Will Change Between Models
  bic_vector_ns[i] <- (n*log(mse_score))+(p*log(n))
}

model5 <- c(mean(mse_vector_ns),mean(bic_vector_ns)) 
model_comp_df <- cbind(model_comp_df, model5)

# Model 6 Natural Splines: Obesity, Eggs, Under, Ani Prod, Veg Products, Ani Fat

mse_vector_ns <- vector() # Reset Vectors
bic_vector_ns <- vector()

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  
  # Model Fit & Predict on Testing Data
  model <- lm(Deaths ~ ns(Obesity,4)+ns(Eggs,4)+ns(Undernourished,4)+
                ns(Animal.Products,5)+ns(Vegetal.Products,5)+
                ns(Animal.fats,4), data = nutrition_cln, subset = train_index)
  predictions <- predict(model, newdata = nutrition_test) 
  
  # MSE Calculation & Input into Vector
  mse_score <- mean((nutrition_test$Deaths-predictions)^2) 
  mse_vector_ns[i] <- mse_score
  
  # BIC Calculation
  n <- nrow(nutrition_test) # For AIC/BIC Calculations
  p <- length(model$coefficients) # Number of Parameters, Will Change Between Models
  bic_vector_ns[i] <- (n*log(mse_score))+(p*log(n))
}

model6 <- c(mean(mse_vector_ns),mean(bic_vector_ns)) 
model_comp_df <- cbind(model_comp_df, model6) 

# Sanity Check Model Natural Splines: Obesity, Animal Products

mse_vector_ns <- vector() # Reset Vectors
bic_vector_ns <- vector()

for (i in 1:length(folds)){
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  nutrition_test <- nutrition_cln[test_index,]
  
  # Model Fit & Predict on Testing Data
  model <- lm(Deaths ~ ns(Obesity,4)+ns(Animal.Products,5), 
              data = nutrition_cln, subset = train_index)
  predictions <- predict(model, newdata = nutrition_test) 
  
  # MSE Calculation & Input into Vector
  mse_score <- mean((nutrition_test$Deaths-predictions)^2) 
  mse_vector_ns[i] <- mse_score
  
  # BIC Calculation
  n <- nrow(nutrition_test) # For AIC/BIC Calculations
  p <- length(model$coefficients) # Number of Parameters, Will Change Between Models
  bic_vector_ns[i] <- (n*log(mse_score))+(p*log(n))
}

mean(mse_vector_ns)
mean(bic_vector_ns) #See That These Values Are Not As Optimal As Comparable Mdl2

# Analyze Full Natural Spline Fit Models:
print(model_comp_df)
```

Taking into account MSE and BIC values, we are left with conflicting conclusions. BIC would favor our first model with a natural spline on obesity. This model has the lowest number of parameters favoring its BIC score. In terms of MSE, it does not perform as well as model 2 which takes a natural spline of obesity and egg fat content. Out of the 6 models we tested (using a quasi-forward step selection method), model 2 returned the lowest MSE score. Since we are most concerned about prediction accuracy over parsimony (though model 2 is still quite parsimonious), our natural spline exercise point towards using a natural spline regression of two predictors: obesity and eggs with knot degrees of freedom of 4 for both. We ran a sanity check by creating a model with obesity (by far the best natural spline predictor) and another strong natural spline predictor (animal fats). This sanity check was to see if there were other possible and more optimal predictor interactions outside of our forward stepwise method. The MSE and BIC for that two predictor model was not as optimal as its forward stepwise counterpart model 2 with obesity and egg as predictors. Moving forward, we'll look into generalized additive methods.

# GAM: Jon (Leave Descriptions Until Last, Just Write Code)


Using the insights from the previous two sections, we now look into possible combinations of different parameter transformations. We are using Generalized additive models to do so, and will test a variety of different combinations of splines and polynomial transformations to see if they can out perform any of the splines or polynomial models on their own. 
```{r}
library(gam)
```

GAM 1 CV
```{r}
# Empty vector for CV preds
gam1_preds <- numeric(length(nutrition_cln$Deaths))

# Empty Data Frame
Degree <- 1:4
gam1_empty_df <- data.frame(Degree)

# CV loop
for (i in 1:length(folds)) {
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  gam1_nutrition_test <- nutrition_cln[test_index,]
  gam1_cv_mse_vector <- vector()
  
  for (j in 1:4) {
  # Define GAM
  gam1 <- gam(Deaths ~ ns(Obesity,4) + ns(Eggs,4)+ poly(Animal.fats, j), 
              data = nutrition_cln, subset = train_index)
  
  # Test set preds
  gam1_test_preds <- predict(gam1, newdata = nutrition_cln[test_index, ])
  
  # MSE Score Calculation
  gam1_cv_mse_vector[j] <- mean((gam1_test_preds - gam1_nutrition_test$Deaths)^2)
  
  }
  
  gam1_empty_df <- cbind(gam1_empty_df, gam1_cv_mse_vector)
}

rowMeans(gam1_empty_df[,-1])


```
Using the two best splines from the previous section, a natural spline on Obesity and a natural spline on Eggs, we test four possible GAMs that add both of those splines with the variable Animal.fats, and vary the polynomial degree of Animal.fats from 1 to 4. The cross validation suggests that the GAM including both splines and Animal.fats of degree 1 is the best performer so far. 


GAM 2 CV
```{r}
# Empty vector for CV preds
gam2_preds <- numeric(length(nutrition_cln$Deaths))

# Empty Data Frame
Degree <- 1:4
gam2_empty_df <- data.frame(Degree)

# CV loop
for (i in 1:length(folds)) {
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  gam2_nutrition_test <- nutrition_cln[test_index,]
  gam2_cv_mse_vector <- vector()
  
  for (j in 1:4) {
  # Define GAM
  gam2 <- gam(Deaths ~ ns(Obesity,4) + ns(Eggs,4) + poly(Obesity, j), 
              data = nutrition_cln, subset = train_index)
  
  # Test set preds
  gam2_test_preds <- predict(gam2, newdata = nutrition_cln[test_index, ])
  
  # MSE Score Calculation
  gam2_cv_mse_vector[j] <- mean((gam2_test_preds - gam2_nutrition_test$Deaths)^2)
  
  }
  
  gam2_empty_df <- cbind(gam2_empty_df, gam2_cv_mse_vector)
}

rowMeans(gam2_empty_df[,-1])

```
This model sees improvements in MSE from the previous model, but the higher degree polynomials on Obesity only worsen the model. The addition of Obesity using degree 1 is the best of the four new models and is identical to the MSE of the model that uses the two natural splines alone. 


GAM 3 CV
```{r}
# Empty vector for CV preds
gam3_preds <- numeric(length(nutrition_cln$Deaths))

# Empty Data Frame
Degree <- 1:4
gam3_empty_df <- data.frame(Degree)

# CV loop
for (i in 1:length(folds)) {
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  gam3_nutrition_test <- nutrition_cln[test_index,]
  gam3_cv_mse_vector <- vector()
  
  for (j in 1:4) {
  # Define GAM
  gam3 <- gam(Deaths ~ ns(Obesity,4) + ns(Eggs,4) + poly(Eggs, j), 
              data = nutrition_cln, subset = train_index)
  
  # Test set preds
  gam3_test_preds <- predict(gam3, newdata = nutrition_cln[test_index, ])
  
  # MSE Score Calculation
  gam3_cv_mse_vector[j] <- mean((gam3_test_preds - gam3_nutrition_test$Deaths)^2)
  
  }
  
  gam3_empty_df <- cbind(gam3_empty_df, gam3_cv_mse_vector)
}

rowMeans(gam3_empty_df[,-1])
```
Once again, we see the higher degree polynomials on Eggs only worsen the model. The addition of Eggs using degree 1 is the best of the four new models and is identical to the MSE of the model that uses the two natural splines alone.


GAM 4 CV
```{r}
# Empty vector for CV preds
gam4_preds <- numeric(length(nutrition_cln$Deaths))

# Empty Data Frame
Degree <- 1:4
gam4_empty_df <- data.frame(Degree)

# CV loop
for (i in 1:length(folds)) {
  train_index <- folds[[i]]
  test_index <- setdiff(1:length(nutrition_cln$Deaths), train_index)
  gam4_nutrition_test <- nutrition_cln[test_index,]
  gam4_cv_mse_vector <- vector()
  
  for (j in 1:4) {
  # Define GAM
  gam4 <- gam(Deaths ~ ns(Obesity,4) + ns(Eggs,4) + poly(Vegetal.Products, j), 
              data = nutrition_cln, subset = train_index)

  # Test set preds
  gam4_test_preds <- predict(gam4, newdata = nutrition_cln[test_index, ])
  
  # MSE Score Calculation
  gam4_cv_mse_vector[j] <- mean((gam4_test_preds - gam4_nutrition_test$Deaths)^2)
  
  }
  
  gam4_empty_df <- cbind(gam4_empty_df, gam4_cv_mse_vector)
}

rowMeans(gam4_empty_df[,-1])
```
Our final set of GAMs uses the same 2 natural splines as before and tests Vegetable products of degrees 1 through 4. This model reports an MSE of 0.001517956 for the fourth degree transformation of Vegetable.Products, making it the best performing model so far out of all of the Polynomials, Splines, and GAMS. The Final model is given by: gam(Deaths ~ ns(Obesity,4) + ns(Eggs,4) + poly(Vegetable.Products, j))


# Part II: Tree-Based Models

## Introduction

In this part of the project, we will run tree-based models to predict AirBnB prices per night as well as guest satisfaction. The data set consists of over fifty thousand observations of AirBnBs around the world, in cities like Amsterdam and Vienna. We will run a variety of classification and regression trees, including random forests and boosting methods. Before running our models, we'll briefly analyze features of interest in our AirBnB data and clean out some of them. 

## Analyze & Clean Data
```{r}
# Load Data
airbnb <- read.csv("Aemf1.csv", header = TRUE, sep = ",")
head(airbnb) 
names(airbnb)

# AirBnB Price Analysis
hist(airbnb$Price, breaks = 50, main = "AirBnB Prices", xlab = "AirBnB Prices") 
summary(airbnb$Price)
head(airbnb$Price[order(airbnb$Price, decreasing = TRUE)], 20) # Top 20 Prices 

# Private Room Analysis: Indicator Variable for Private Rooms (1 = Yes, 0 = No)
table(airbnb$Private.Room)/length(airbnb$Private.Room) # 70 - 30 Balance

# Person Capacity Analysis
hist(airbnb$Person.Capacity, breaks = 10, main = "AirBnB Person Capacity", 
     xlab = "AirBnB Prices") 
summary(airbnb$Person.Capacity) 

# Superhost (Awarded to Top Rated Hosts) Capacity Analysis
table(airbnb$Superhost)/length(airbnb$Superhost) # 70 - 30 Balance

# Multiple Rooms Analysis
table(airbnb$Multiple.Rooms)/length(airbnb$Multiple.Rooms) # 70 - 30 Balance

# Business Analysis
table(airbnb$Business)/length(airbnb$Business) 

# For Remainder Quantitative Variables Use For Loop of Histograms
remainder <- c("Cleanliness.Rating", "Guest.Satisfaction", "Bedrooms",                    "City.Center..km.", "Metro.Distance..km.", "Attraction.Index", "Normalised.Attraction.Index", "Restraunt.Index", "Normalised.Restraunt.Index")

# Plot Histograms for Remainder Variables
for (variable in remainder){
  print(hist(airbnb[[variable]], breaks = 20, main = variable, xlab = variable))
  print(summary(variable))
}
```

In the analysis of our variables, we did not look into Room.Type and Shared.Room variables since they are both captured in the indicator variable Private.Room. We won't use the attraction and restaurant index since their distributions are not normally distributed. The data set has normally distributed values for those features. Before we run feature selection algorithms - in order to make our dataset even more lean (less features) - we will have to convert some of our features (such as Private.Room) into indicators. 

We'd also like to create a classifier variable out of guest satisfaction. Interestingly, the median guest satisfaction score for the AirBnB homes in our data aset is 95 out of 100. Empirically, this makes sense since AirBnB customers (just like Uber riders and other consumers in the gig economy) are naturally inclined to give their hosts a good rating - even if the service is not quite up to par. This median threshold will thus be a separating point between "good" AirBnBs and excellent AirBnBs. We'll create a new indicator/classifier variable to highlight AirBnBs above and below the median. 

```{r}
# Convert Variables into Indicators (1 = Yes, 0 = No)

# Classifier Variable: Guest Satisfaction, r = response variable
airbnb$Guest.Satisfaction.r <- ifelse(airbnb$Guest.Satisfaction > 
                                        median(airbnb$Guest.Satisfaction), 
                                      1,0) 
summary(airbnb$Guest.Satisfaction.r) # 47% are 1s

# Private Room into Indicator
airbnb$Private.Room.ind <- ifelse(airbnb$Private.Room == "True", 1,0)
summary(airbnb$Private.Room.ind) # Check to see if proportions are right
table(airbnb$Private.Room)/length(airbnb$Private.Room) # Yes, they match

# Superhost into Indicator
airbnb$Superhost.ind <- ifelse(airbnb$Superhost == "True", 1,0)
summary(airbnb$Superhost.ind) 
table(airbnb$Superhost)/length(airbnb$Superhost)
```

## Labelling Data Sets for Each of Our Two Problems
```{r}
# Use Dplyr To Remove Features We Won't Use 
library(dplyr)

# Data Set for the Regression Problem, R = regression
airbnb.cln.r <- airbnb %>% select(-c("City", "Day","Room.Type", "Shared.Room",
                                     "Private.Room", "Superhost", "Guest.Satisfaction.r",
                                   "Attraction.Index", "Restraunt.Index")) 
head(airbnb.cln.r)

# Data set for Classification Problem (Guest Satisfaction As Response), C = class
airbnb.cln.c <- airbnb %>% select(-c("City", "Day","Room.Type", "Shared.Room",
                                     "Private.Room", "Superhost", "Guest.Satisfaction",
                                   "Attraction.Index", "Restraunt.Index")) 


head(airbnb.cln.c) # Check to see if all variables we need are in  

# Check for any NA Values
any(is.na(airbnb.cln.c))
any(is.na(airbnb.cln.r))
```

## Feature Selection & Additional Cleaning
```{r}
# Boruta Algorithm
library(Boruta)
library(randomForest) 

# Boruta for Regression Problem
boruta_airbnb_reg <- Boruta(Price ~., data = airbnb.cln.r, doTrace = 2, 
                            randomForest = TRUE)
plot(boruta_airbnb_reg, las = 2, xlab = "", main = "Boruta Algorithm for Regression Problem") 

# Boruta for Classification Problem
boruta_airbnb_class <- Boruta(Guest.Satisfaction.r ~., 
                              data = airbnb.cln.c, doTrace = 2, 
                              randomForest = TRUE) 
plot(boruta_airbnb_class, las = 2, xlab = "", main = "Boruta Algorithm for Classification Problem")
boruta_airbnb_class
```

## Create Folds 
```{r}
set.seed(1209745)
library(caret)
Fold <- createFolds(airbnb.cln.r$Price, k = 5, returnTrain = TRUE) # Train Index 

# See that this fold can also be used for our classification data set
head(airbnb.cln.c[Fold[[2]],])
```











